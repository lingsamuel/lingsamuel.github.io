<!DOCTYPE html><html lang="zh" itemscope="" itemtype="http://schema.org/WebPage"><head><meta charset="utf-8"><meta http-equiv="x-ua-compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><title>一次 K8s 集群修复日志 - Hakurei Shrine</title><link rel="alternate" href="https://lingsamuel.github.io/index.xml" type="application/rss+xml" title="Hakurei Shrine"><link href="https://lingsamuel.github.io/img/favicon.ico" rel="icon" type="image/x-icon" alt="favicon"><link rel="preconnect" href="https://fonts.gstatic.com"><link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;700&amp;family=Source+Serif+Pro:wght@400;700&amp;display=swap" rel="stylesheet"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Fira+Code:wght@300;400;500;600;700&amp;family=Noto+Sans+SC:wght@100;300;400;500;700;900&amp;family=Noto+Serif+SC:wght@200;300;500;600;900&amp;family=Source+Serif+Pro:ital,wght@0,200;0,300;0,600;0,900;1,200;1,300;1,400;1,600;1,700;1,900&amp;display=swap" media="screen" as="style" onload="this.rel='stylesheet'"><link rel="stylesheet" href="https://lingsamuel.github.io/css/main.min.css" media="screen"><link rel="stylesheet" href="https://lingsamuel.github.io/css/book.min.css" media="screen" as="style" onload="this.rel='stylesheet'"><link rel="stylesheet" href="https://lingsamuel.github.io/css/quotes.min.css" media="screen" as="style" onload="this.rel='stylesheet'"><script type="application/javascript">var doNotTrack=!1;doNotTrack||(window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga('create','UA-89095899-1','auto'),ga('send','pageview'))</script><script async="" src="https://www.google-analytics.com/analytics.js"></script></head><body id="body"><nav id="navbar"><div class="nav"><div class="navbar-header"><img src="https://lingsamuel.github.io/img/favicon.ico" class="favicon" alt="favicon"><a href="https://lingsamuel.github.io/">
Hakurei Shrine</a></div><div class="navbar-menu"><ul class="navbar-list"><li><a title="Articles" href="/page">Articles</a></li><li><a title="Reviews" href="/review">Reviews</a></li><li><a title="Feeds" href="/feed">Feeds</a></li><li><a title="Kubernetes" href="/k8s">Kubernetes</a></li><li><a title="About" href="/about">About</a></li><li><a title="Categories" href="/categories">Categories</a></li><li><a title="Tags" href="/tags">Tags</a></li></ul></div></div></nav><div class="flex-row-container"><div class="container container-single" id="mainContainer" role="main"><div class="container-single-main"><header class="header post-header"><div class="row"><div class="post-title"><h1>一次 K8s 集群修复日志</h1></div><div class="post-meta"><span class="author">Ling Samuel</span>
•
<span class="date">2020-07-17</span>
•
<span class="categories"><a href="https://lingsamuel.github.io/categories/kubernetes/">kubernetes</a>&nbsp;</span>
•
<span class="tags"><a href="https://lingsamuel.github.io//tags/linux/">linux</a>&nbsp;
<a href="https://lingsamuel.github.io//tags/k8s/">k8s</a>&nbsp;</span></div></div></header><div class="row"><article role="main" class="post-content"><p>离谱。</p><h2 id="起因">起因</h2><p>集群过载一段时间之后，K8s 的 <code>node</code> 突然统统消失了。</p><p>随后节点 1 和节点 2 恢复，其余节点始终没有自愈。</p><h2 id="恢复">恢复</h2><p>重启 <code>kubelet</code> 可以恢复节点，但是不重启的节点始终没有自愈。</p><p>查看 <code>kubelet</code> 的日志发现大量 <code>Unexpected watch close - watch lasted less than a second and no items received</code> 和 <code>use of closed network connection</code> 的错误日志，貌似<code>kubelet</code> 用了死掉的链接。</p><p>从 <a href="https://github.com/kubernetes/kubernetes/issues/87615" style="font-family: &quot;source serif pro&quot;, &quot;noto serif sc&quot;, &quot;source serif&quot;, &quot;source han serif sc&quot;, &quot;source han serif cn&quot;, serif;">(1.17) Kubelet won’t reconnect to Apiserver after NIC failure (use of closed network connection)</a> <a href="https://github.com/kubernetes/kubernetes/issues/87615#issuecomment-647915537">来看</a>，这还是个和 Golang 有关的 <a href="https://github.com/golang/go/issues/39750">bug</a>……</p><p>期间由于部分节点提前恢复，导致大量的 pod 调度到其上，导致资源占满，DaemonSet <code>calico</code> 的 pod 始终 Pending，kill 掉大量 pod 即可恢复。</p><p>随后发现 <code>calico</code> Unhealthy，网络依旧存在问题。</p><p>通过 <code>calicoctl node status</code> 发现节点 2 的 IP 因为神秘原因<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> 错误，并非 <code>eth0</code> 的 IP。</p><div class="highlight"><pre class="chroma"><code class="language-bash" data-lang="bash"><span class="nb">export</span> <span class="nv">ETCD_KEY_FILE</span><span class="o">=</span><span class="s2">"/etc/kubernetes/ssl/etcd-key.pem"</span>
<span class="nb">export</span> <span class="nv">ETCD_CERT_FILE</span><span class="o">=</span><span class="s2">"/etc/kubernetes/ssl/etcd.pem"</span>
<span class="nb">export</span> <span class="nv">ETCD_CA_CERT_FILE</span><span class="o">=</span><span class="s2">"/etc/kubernetes/ssl/ca.pem"</span>
<span class="nb">export</span> <span class="nv">ETCD_ENDPOINTS</span><span class="o">=</span><span class="s2">"https://IP1,https://IP2,https://IP3:2379"</span>
calicoctl get node -o yaml &gt; calico-node2.yaml
<span class="c1"># 修正 calico-node2.yaml</span>
calicoctl replace -f ./calico-node2.yaml
</code></pre></div><p>修复 IP 即可恢复。</p><section class="footnotes" role="doc-endnotes"><hr><ol><li id="fn:1" role="doc-endnote"><p>这是由于节点上有人偷偷给 docker 添加了一个 network，而 calico 工作在 autodetect 模式的缘故。 <a href="#fnref:1" class="footnote-backref" role="doc-backlink">↩︎</a></p></li></ol></section><div id="refContainer"></div></article></div></div><div class="row"><div class="post-pager"><ul class="pager"><li class="previous"><a href="https://lingsamuel.github.io/page/2020-07-14-customization/">← GNOME Customization Backup</a></li><li class="next"><a href="https://lingsamuel.github.io/page/2020-07-17-gnome-cannot-unlock-bug/">GNOME 的无法解锁 Bug →</a></li></ul></div></div></div><div id="toc"><aside><nav id="TableOfContents"><ul><li><a href="#起因">起因</a></li><li><a href="#恢复">恢复</a></li></ul></nav></aside></div></div><footer id="footer"><div class="footer"><p>Copyright © 2021
<a href="https://lingsamuel.github.io">Ling Samuel</a> • Powered by <a href="https://gohugo.io">Hugo v0.82.1</a> • Theme <a href="https://github.com/lingsamuel/purity">Purity</a> •
Hosted by GitHub</p></div></footer><link rel="stylesheet" href="https://lingsamuel.github.io/css/mermaid.min.css" media="screen" as="style" onload="this.rel='stylesheet'"></body></html>